---
title: "Week 3"
author: "Aditya"
date: "6/15/2020"
output: html_document
---

# Subsetting and sorting

### **Subsetting**

**Creating Dataset**
```{r}

set.seed(13435)
X <- data.frame("var1" = sample(1:5),"var2" = sample(6:10), "var3" = sample(11:15))
X <- X[sample(1:5),]; X$var2[c(1,3)] = NA
X


```

Subset an specific column by doing
```{r}

X[,1]

```

or, by doing with column name
```{r}

X[,'var1']

```

or, i can select specific rows, and columns by
```{r}

X[1:3,'var2']

```

or with columns index
```{r}

X[c(1,3,5),c(3,1)]

```

### **Logicals**

Selecting all rows which var1 value is less than or equal to 3 and var3 value is greater than 11
```{r}

X[(X$var1<=3 & X$var3>11),]

```

similarly,
```{r}

X[(X$var1 <=3 | X$var3>15),]

```

### **Dealing with missing values**

which command gets rid of the missing values
```{r}

X[which(X$var2 > 8),]

```

### **Sorting**
```{r}

sort(X$var1)

```

by default it's in increasing order. we can sort it in decreasing order to, by passing decreasing parameter
```{r}

sort(X$var1, decreasing = T)

```

while sorting with NA values, they are put first by default. If selected na.last = TRUE, then they will be put last
```{r}

sort(X$var2,na.last = T)

```

### **Ordering**

We can actually order a data frame by one or more particular columns
```{r}

X[order(X$var1),]

```

sorting with multiple variables. In these case, sort first with value 1, if there is a tie then, sort by value 2
```{r}

X[order(X$var2,X$var3,na.last = F),]

```

### **Ordering with plyr**

We can do the same thing with the plyr library
```{r}

library(plyr)
arrange(X,var1)

```

or to arrange in decreasing order
```{r}

arrange(X,desc(var1))

```

### **Adding rows and columns in the data frame**

```{r}

X$var4 <- rnorm(5)
X

```

also can add with the cbind command
```{r}

Y <- cbind(X,hakunaMatata=rnorm(5))
Y
```

can add a new row with the rbind command
```{r}

Y <- rbind(Y, rnorm(5))
Y

```

# Summarizing the data

### **Downloading the dataset**

```{r}

if(!file.exists('restaurants.csv'))
{
  fileURL <-'https://data.baltimorecity.gov/api/views/k5ry-ef3g/rows.csv?accessType=DOWNLOAD'
  download.file(fileURL, destfile = 'restaurants.csv',method = 'curl')
}
restData <- read.csv('restaurants.csv')

```

Checking the data with head. By default n = 6, but we can set n to view the data.
```{r}

head(restData, n =3)

```

To view the column names, there are two methods.
```{r}

names(restData)
# or,
colnames(restData)

```

Both of them will return a list.

Also we can view data, from tail of the dataset
```{r}

tail(restData, n = 5)

```

### **Making Summary**


```{r}

summary(restData)

```

We can use str command to see more information about the data frame. The depth, type, and demo.
```{r}

str(restData)

```

### **Quantiles**

Quantiles on quantitative variables
```{r}

quantile(restData$councilDistrict, na.rm = T)

```

Quantile values with different probabilities
```{r}

quantile(restData$councilDistrict, prob = c(0.5,0.75, 0.9))

```

### **Tables**

We can also make table
```{r}

table(restData$zipCode, useNA = 'ifany')

```

Here, we can see how many zipcode are there with what value. For example 21201 has 136 entries and so on. The parameter useNA will NA value in the end if there is any NA value, because by default it doesn't count the missing values

We can also make 2D tables.
```{r}

table(restData$councilDistrict, restData$zipCode)

```

We can actually view relationship between different data using this 2D table data.

### **Missing values**

```{r}

sum(is.na(restData$councilDistrict))

```

or we can use any() function
```{r}

any(is.na(restData$Location.1))

```

the any() function checks if there is any FALSE in the given **logical array**

```{r}

all(restData$zipCode > 0)

```

There was a negative zipcode. Remember?

### Row and column sums

```{r}

colSums(is.na(restData))

```

Check if there are any missing values in the entire dataset

```{r}

sum(is.na(restData))

```

### **Values with specific characteristics**

```{r}

table(restData$zipCode %in% c('21212') )

```
```{r}

table(restData$zipCode %in% c('21212','21213'))

```

We can also get subset of the data using same procedure
```{r}

restData[restData$zipCode %in% c('21212','21213'),  ]

```

### **Cross tabs**

Can view Data from datasets with the summary
```{r}

data("UCBAdmissions")
DF <- as.data.frame(UCBAdmissions)
summary(DF)

```

Cross tabs. Breaking down Freq by gender and admit column data
```{r}

xt<- xtabs(Freq ~ Gender + Admit, data = DF)
xt

```

We can even crosstab for larger amount of data and variables. But it is hard to see. warpkreaks(standard dataset) break with all other columns
```{r}

warpbreaks$replicate <- rep(1:9, len = 54)
xt = xtabs(breaks ~., data = warpbreaks)
xt

```

We can actually Flat the output. So that, we can see the data in a more compact format
```{r}

ftable(xt)

```

### **Dataset Size**

Size of the dataset in memory
```{r}

fakeData = rnorm(1e5)
object.size(fakeData)

```

Even we can print the size in different scale
```{r}

print(object.size(fakeData),units = 'MB')

```

# Creating variables

creating variables for data analysis. Like if there is any missing values, if there is any value greater than some value, predicting values etc.

### **Creating sequence**

using by command, a sequence can be created containing a constant distance
```{r}

s1 <- seq(1,10,by =2); s1

```

similarly we can give seq() length, and min max values. It'll then create exactly that length sequence
```{r}

s2 <- seq(1,10,length = 3); s2

```

or, we can create a sequence og length equal to another list
```{r}

x <- rnorm(5)
seq(along=x)

```

### **Subsetting variables**

Subsetting the restaurants that are near me
```{r}

restData$nearMe = restData$neighborhood %in% c('Roland Park','Homeland')
table(restData$nearMe)

```

Subsetting restaurants with wrong zipcode. Binary variable
```{r}

restData$zipWrong <- ifelse(restData$zipCode<0,T,F)
table(restData$zipWrong, restData$zipCode <0)
table(restData$zipWrong, restData$zipCode >0)

```

### **Creating categorical variables**

We can create a group of zipcodes.
```{r}

restData$zipGroups <- cut(restData$zipCode, breaks = quantile(restData$zipCode))
table(restData$zipGroups)

```

Viewing the zipcodes
```{r}

table(restData$zipGroups,restData$zipCode)

```

We can even cut the groups into more segments using the cut2() function of the Himsc library
```{r}

library(Hmisc)
restData$zipGroups = cut2(restData$zipCode, g =5)
table(restData$zipGroups)

```

### **Creating factor variables**

```{r}

restData$zcf <- factor(restData$zipCode)
restData$zcf[1:10]

```

### **Levels**
```{r}

yesno <- sample(c('yes','no'), size = 10, replace = TRUE)
yesnof <- factor(yesno, levels = c('yes','no'))
relevel(yesnof, ref = 'yes')

as.numeric(yesnof)

```

### **Creating new DataFrame with new variable**

Creating a copy of restData but adding another varibale called zipGroups. mutate function is under plyr library
```{r}

restData2 <- mutate(restData,zipGroups = cut2(zipCode,g=4))
table(restData2$zipGroups)

```

### **Setting significance**

```{r}

x <- rnorm(10)
x
signif(x,digits = 2)

```

# Reshaping Data

```{r}

library(reshape2)
head(mtcars)

```

### **Melting Data Frames**
```{r}

mtcars$carname <- rownames(mtcars)
carMelt <- melt(mtcars, id = c('carname','gear','cyl'),measure.vars = c('mpg','hp'))
head(carMelt,n=3)

```

```{r}

tail(carMelt, n=3)

```

This actually melted rest of the data, with rest of the variables. So it became, tall and skinny

### **Casting data frames**

```{r}

cylData <- dcast(carMelt, cyl ~ variable)
cylData

```
We can even tell dacast to take the mean of each value
```{r}

cylData <- dcast(carMelt, cyl ~ variable, mean)
cylData

```

### **Averaging values**

```{r}

head(InsectSprays)

```

Take sum on groups of values
```{r}

tapply(InsectSprays$count, InsectSprays$spray, sum)

```

We can split dataset according to a values with the split command
```{r}

spIns <- split(InsectSprays$count, InsectSprays$spray)
spIns

```

Another way is lapply
```{r}

sprCount = lapply(spIns, sum)
sprCount

```

```{r}
# Came from lapply
unlist(sprCount)
sapply(spIns, sum)

```

Can be done with plyr package also

```{r}

ddply( InsectSprays, .(spray), summarise, sum = sum(count)  )

```

creating new variable
```{r}

spraySums <- ddply(InsectSprays, .(spray), summarise, sum = ave(count,FUN = sum))
dim(spraySums)

```

So, what's happening here is, we are applying on InsectSpray, according to spray column, and summarising it with sum = ave function, whose sub function is sum. So we get the same dimension as the InsectSprays

```{r}

head(spraySums)

```

So every time we see an A we also see the total sum of the A's

# dyplr

```{r}

library(dplyr)

```

```{r}

chicago <- readRDS('chicago.rds')
dim(chicago)

```

```{r}

names(chicago)

```

### **Subsetting the columns**

One nice thing is that in the select function of dplyr, we can use column namse as indexes. For example if we want to selcet from city to dptp, then
```{r}

head(select(chicago, city:dptp))

```

we can select other way. select exerything except city -> dptp. Just add a minus sign

```{r}

head(select(chicago, -(city:dptp)))

```

similar code in R. But we need to find out the indices first.
```{r}

i<- match('city',names(chicago))
j<- match('dptp', names(chicago))
chicago[1:6,-(i:j)]

```

### **Subsetting the rows**

We can subset the rows with the dplyr filter function
```{r}

chic.f <- filter(chicago, pm25tmean2 > 30)
head(chic.f,10)

```

Can add multiple rows to do complex sequence
```{r}

chic.f <- filter(chicago, pm25tmean2 >30 & tmpd >80)
head(chic.f)

```

### **Rearraning the rows based on the values in a column**

Arranging the data frame according to date. Lowest date in the top, an increasingly to the bottom
```{r}

chicago <- arrange(chicago, date)
head(chicago)
tail(chicago)

```

Also can arrange them in decreasing order. Just use the desc() function
```{r}

chicago <- arrange(chicago, desc(date))
head(chicago)

```

### **Renaming variables**
Renaming a variable is quit hard in R. But there is a function in dplyr that makes it easy. For example we will rename the pm25tmean2 to pm25 and dptp to dewpoint
```{r}

chicago <- rename(chicago, pm25 = pm25tmean2, dewpoint = dptp)
names(chicago)

```

### **Creating new varibles**
mutate is used to create new variables. Here pm25detrend varibale is created
```{r}

chicago <- mutate(chicago, pm25detrend = pm25 - mean(pm25, na.rm = T))
head(chicago)

```

### **Split rows with categorical variables**

```{r}

chicago <- mutate(chicago, tempcat = factor(1*(tmpd>80), labels = c('cold','hot')))
hotcold <- group_by(chicago, tempcat)
hotcold

```

```{r}

summarize(hotcold, pm25 = mean(pm25), o3 = max(o3tmean2), no2 = median(no2tmean2))

```

We can even summarize the dataset with year.
In that case first we have to create a new variable in the dataset called year
Then group by year
And then summarize
```{r}

chicago <- mutate(chicago, year = as.POSIXlt(date)$year + 1900)
years <- group_by(chicago, year)
summarize(years, pm25 = mean(pm25, na.rm = T), o3 = max(o3tmean2), no2 = median(no2tmean2))

```

### **Pipeline operator**

Pipeline operator allows us to create a pipeline of operations.
```{r}

chicago %>% mutate(month = as.POSIXlt(date)$mon +1) %>% group_by(month) %>% summarize(pm25 = mean(pm25, na.rm = T), o3 = max(o3tmean2),  no2 = median(no2tmean2)) %>% head(3)

```

# Merging Data

Merging Data is needed when multiple Data packaged are loaded and we need to merge them

### **Downloading the Data**
```{r}

if(!file.exists("reviews.csv"))
{
  fileUrl1 = "https://github.com/Aitto/courses/blob/master/03_GettingData/03_05_mergingData/data/reviews.csv"
  fileUrl2 = "https://dl.dropboxusercontent.com/u/7710864/data/solutions-apr29.csv"
  download.file(fileUrl1,destfile="reviews.csv",method="curl")
  download.file(fileUrl2,destfile="solutions.csv",method="curl")
}

reviews = read.csv("reviews.csv")
solutions <- read.csv("solutions.csv")
head(reviews,2)

```

```{r}

head(solutions,2)

```

Merge with solution_id from 1st dataset and id from second dataset
```{r}

mergeData = merge(reviews, solutions, by.x = 'solution_id', by.y = 'id', all = T)
head(mergeData)

```

See what rows are common
```{r}

intersect(names(reviews), names(solutions))

```

By default it merges with all common columns of the same name
```{r}

merge(reviews,solutions,all = T) %>% head()

```

We can use join from plyr dataset. It's a bit faster than merge, but it can only join datasets with common names. It's less featured than merge.
```{r}

df1 <- data.frame(id = sample(1:10), x = rnorm(10))
df2 <- data.frame(id = sample(1:10), y = rnorm(10))
arrange(join(df1,df2),id)

```

Also can merge multiple dataframes
```{r}

df1 <- data.frame(id = sample(1:10), x = rnorm(10))
df2 <- data.frame(id = sample(1:10), y = rnorm(10))
df3 <- data.frame(id = sample(1:10), z = rnorm(10))

list(df1,df2,df3) %>% join_all()

```

